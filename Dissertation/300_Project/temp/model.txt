import csv
import os
import torch
import numpy as np
import pytorch_lightning as pl
from typing import List, Optional, Dict, Tuple
from ml4floods.preprocess.worldfloods import normalize
from ml4floods.models.config_setup import  AttrDict

from ml4floods.models.utils import losses, metrics
from ml4floods.models.architectures.baselines import SimpleLinear, SimpleCNN
from ml4floods.models.architectures.unets import UNet, UNet_dropout
from ml4floods.models.architectures.hrnet_seg import HighResolutionNet
from ml4floods.models.architectures.segnet import SegNet
from ml4floods.data.worldfloods.configs import COLORS_WORLDFLOODS, CHANNELS_CONFIGURATIONS, BANDS_S2, COLORS_WORLDFLOODS_INVLANDWATER, COLORS_WORLDFLOODS_INVCLEARCLOUD
from pytorch_lightning.loggers import WandbLogger
from ml4floods.data.utils import get_filesystem
from sklearn.cluster import MiniBatchKMeans

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier

import numpy as np
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as F



class WorldFloodsModel(pl.LightningModule):
    """
    Model to do multiclass classification.
    It expects ground truths y (B, H, W) tensors to be encoded as: {0: invalid, 1: clear, 2:water, 3: cloud}
    The preds (model.forward(x)) will produce a tensor with shape (B, 3, H, W)
    """
    def __init__(self, model_params: AttrDict, normalized_data:bool=True):
        super().__init__()
        self.save_hyperparameters()
        h_params_dict = model_params.get('hyperparameters', {})
        self.num_class = h_params_dict.get('num_classes', 4)
        self.network = configure_architecture(h_params_dict)
        self.weight_per_class = torch.tensor(h_params_dict.get('weight_per_class',[1.0 for _ in range(self.num_class)]),
                                            dtype=torch.float32,
                                            ).to(self.device)
        
        """class_counts = torch.tensor([85636729, 725670728, 44626862, 97745553], dtype=torch.float32)
        class_weights = 1.0 / class_counts
        class_weights /= class_weights.sum()  # Normalize
        self.weight_per_class = class_weights.to(self.device)
        print(f"class weights: {self.weight_per_class}")"""
        self.normalized_data = normalized_data
        self.num_prototypes = 200 # number of prototypes per class

        #changes
        #prototype layer initalisations
        # Add prototype layer
        # prototype vector is a representation of a class so the model can learn by comparing pixel to vector
        # act as clusters
        self.latent_dim = h_params_dict.get('latent_dim', 64)
        self.prototype_vectors = torch.nn.Parameter(
                torch.nan_to_num(
                    torch.randn(
                        self.num_class, self.num_prototypes, self.latent_dim) / np.sqrt(self.latent_dim)
            )  # Initialize randomly
        )
        torch.nn.init.xavier_uniform_(self.prototype_vectors)

        self.raw_dim = 13  # Sentinel-2 images have 13 spectral bands
        self.prototype_vectors_raw = torch.nn.Parameter(
            torch.nan_to_num(torch.randn(self.num_class, self.num_prototypes, self.raw_dim))
        )
        torch.nn.init.xavier_uniform_(self.prototype_vectors_raw)
        
        # Initialize prototype labels (Each prototype corresponds to its class index)
        self.prototype_labels = torch.arange(self.num_class).repeat_interleave(self.num_prototypes).to(self.device)


        #print(f"‚úÖ Model Initialized | Prototypes Shape: {self.prototype_vectors.shape}")
        self.rules = {}
        self.rule_storage = []

        """self.weight_per_class = torch.Tensor(
            h_params_dict.get('weight_per_class', [1 for _ in range(self.num_class)]), 
            device=self.device
        )"""

        # learning rate params
        self.lr = h_params_dict.get('lr', 1e-4)
        self.lr_decay = h_params_dict.get('lr_decay', 0.5)
        self.lr_patience = h_params_dict.get('lr_patience', 2)

        self.train_epoch_metrics = {
            "train_total_loss": [],
            "train_bce_loss": [],
            "train_prototype_loss": [],
            "train_recall_land": [],
            "train_recall_water": [],
            "train_recall_cloud": [],
            "train_iou_land": [],
            "train_iou_water": [],
            "train_iou_cloud": []
        }
        
        #label names setup
        self.label_names = h_params_dict.get('label_names', [i for i in range(self.num_class)])


    def training_step(self, batch: Dict, batch_idx) -> float:
        """
        Args:
            batch: includes
                x (torch.Tensor): (B, C, W, H), input image
                y (torch.Tensor): (B, 1, W, H) encoded as {0: invalid, 1: land, 2: water, 3: cloud}
        """
        x, y = batch['image'], batch['mask']
        x = torch.nan_to_num(x).to(dtype=torch.float32)
        y = torch.nan_to_num(y).to(dtype=torch.long)
        #print(f"Unique mask values seen in training step: {torch.unique(y)}")
        latent_features = self.extract_latent_features(x)
        raw_features = x
        #print(f"üõ†Ô∏è DEBUG: raw_features shape before loss computation: {raw_features.shape}")
        #print(latent_features)
        
        #print("DEBUG - Unique Labels at Init:", torch.unique(y, return_counts=True))

        # Re-cluster prototypes at the start of each epoch
        n = 50;
        if batch_idx % n == 0: # update every n batches
            self.update_prototypes(latent_features, raw_features, y)

        total_loss, bce_loss, prototype_loss = self.compute_losses(latent_features, y)

        predictions = self.classify(latent_features, raw_features)
        #print(f"Shape of predictions from classify(): {predictions.shape}")  # Debugging print

        y = y.squeeze(1)  # Ensure shape matches predictions
        pred_categorical = predictions.long()

        #print(f"Shape of pred_categorical before unpacking: {pred_categorical.shape}")
        B, HW = pred_categorical.shape
        H, W = int(HW ** 0.5), int(HW ** 0.5)  # Assuming square images
        pred_categorical = pred_categorical.view(B, H, W)

        cm_batch = metrics.compute_confusions(y, pred_categorical, num_class=self.num_class, remove_class_zero=True)
        recall = metrics.calculate_recall(cm_batch, self.label_names)
        iou_dict = metrics.calculate_iou(cm_batch, self.label_names)

        # Accumulate metrics over the epoch
        self.train_epoch_metrics["train_total_loss"].append(total_loss.detach().cpu().item())
        self.train_epoch_metrics["train_bce_loss"].append(bce_loss.detach().cpu().item())
        self.train_epoch_metrics["train_prototype_loss"].append(prototype_loss.detach().cpu().item())

        for k in recall.keys():
            self.train_epoch_metrics[f"train_recall_{k}"].append(recall[k])
        for k in iou_dict.keys():
            self.train_epoch_metrics[f"train_iou_{k}"].append(max(iou_dict[k], 0))
        """for k, v in iou_dict.items():
            print(f"IoU calculation for class '{k}': {v}")"""

        return total_loss
    
    def extract_latent_features(self, x):
        logits = self.network.get_penultimate_layer(x)
        B, C, H, W = logits.shape
        #print(f"Penultimate logits shape: {logits.shape}")
        latent_features = logits.view(B, C, -1).permute(0, 2, 1)
        #print(f"Latent features shape after reshaping: {latent_features.shape}")
        latent_features = latent_features / torch.norm(latent_features, p=2, dim=-1, keepdim=True)
        latent_features = torch.nan_to_num(latent_features, nan=0.0).to(dtype=torch.float32)
        return latent_features

    def update_prototypes(self, latent_features, raw_features, y, momentum=0.2, min_samples=10):
        """
        update prototypesd useing MiniBatchKMeans per class

        """
        # flatten y to match latent_features
        B, HW, latent_dim = latent_features.shape

        y = y.view(B, -1) # [b, hw]
        latent_features = latent_features.contiguous().view(B * HW, latent_dim)
        latent_features = torch.nn.functional.normalize(latent_features, p=2, dim=-1)
        latent_features = torch.nan_to_num(latent_features, nan=0.0)
        raw_features = raw_features.view(B * HW, -1)
        y = y.view(-1) # [b*hw]

        updated_prototypes  = self.prototype_vectors.clone()


        for class_id in range(self.num_class):
            #select features for class
            class_mask = y == class_id
            latent_class_features = latent_features[class_mask]
            latent_class_features_np = latent_class_features.detach().cpu().numpy()
            raw_class_features = raw_features[class_mask]
            
            num_valid_samples = latent_class_features.size(0)
            if num_valid_samples < min_samples:
                print(f"Skipping update for class {class_id}: Too few samples ({num_valid_samples}).")
                continue 

            if latent_class_features.size(0) == 0:
                continue

            num_clusters = self.num_prototypes
            #assert not torch.isnan(latent_class_features).any(), f"NaN detected in latent features for class {class_id}"

            latent_class_features = torch.nan_to_num(latent_class_features, nan=0.0)

            #print(f"üîπ Class {class_id} | Num Clusters: {num_clusters}")

            k = MiniBatchKMeans(n_clusters = num_clusters, batch_size=128, max_iter=300, init="k-means++")
            try:
                #store cluster centers
                k.fit(latent_class_features_np)
                cluster_centers = torch.tensor(k.cluster_centers_, device=latent_features.device)
                cluster_centers_raw = raw_class_features.mean(dim=0)

                num_actual_clusters = cluster_centers.shape[0]  # Sometimes smaller than num_clusters

                # Apply momentum-based updates
                updated_prototypes[class_id, :self.num_prototypes] = (
                    momentum * self.prototype_vectors[class_id, :self.num_prototypes] + (1 - momentum) * cluster_centers
                )

                updated_prototypes = torch.nn.functional.normalize(updated_prototypes, p=2, dim=-1)

                self.prototype_vectors_raw.data[class_id, :num_actual_clusters] = (
                    momentum * self.prototype_vectors_raw[class_id, :num_actual_clusters] +
                    (1 - momentum) * cluster_centers_raw.expand(num_actual_clusters, -1)
                )

                self.add_rule(class_id, cluster_centers)
            except ValueError as e:
                print(f"Skipping prototype update for class {class_id} due to clustering issue: {e}")

        self.prototype_vectors.data.copy_(torch.nan_to_num(updated_prototypes, nan=0.0))


    """=================Interpretable Funcitons==================="""

    def classify(self, latent_features, raw_features):
        """ use protoype distances and interpretable rules to class pixels """
        #print(f"üìè DEBUG: classify | raw_features shape BEFORE accessing bands: {raw_features.shape}")
        B, HW, latent_dim = latent_features.shape
        latent_features = latent_features.contiguous().reshape(B * HW, latent_dim)

        self.prototype_vectors = torch.nn.Parameter(torch.nan_to_num(self.prototype_vectors, nan=0.0).to(dtype=torch.float32))

        distances = torch.cdist(
            torch.nan_to_num(latent_features, nan=0.0).to(dtype=torch.float32), 
            torch.nan_to_num(self.prototype_vectors.view(-1, latent_dim), nan=0.0)
        )
        distances = distances.view(B, HW, self.num_class, self.num_prototypes)

        similarity_scores = torch.exp(-distances ** 2)
        similarity_scores /= similarity_scores.sum(dim=-2, keepdim=True)

        # Find the k-nearest prototypes for each pixel
        distances_flat = distances.view(B, HW, -1)
        _, nearest_prototypes = distances_flat.topk(k=10, dim=-1, largest=False)
        
        class_votes = nearest_prototypes // self.num_prototypes  # Convert prototype index to class, Shape: [B, HW, 10]
        proto_indices = nearest_prototypes % self.num_prototypes
        
        weighted_votes = torch.zeros((B, HW, self.num_class), device=latent_features.device)

        proto_indices_expanded = proto_indices.unsqueeze(2)

        for class_id in range(self.num_class):
            class_mask = (class_votes == class_id).float()  # Convert boolean mask to float, Shape: [B, HW, 10]
            
            # Gather correct similarity scores for top-10 nearest prototypes
            gathered_similarities = torch.gather(
                similarity_scores[:, :, class_id, :],  # Shape: [B, HW, num_prototypes]
                -1,
                proto_indices_expanded.squeeze(2)  # Ensure shape matches similarity_scores
            )
            
            vote_sum = (class_mask * gathered_similarities).sum(dim=-1)  # Sum across top 10, Shape: [B, HW]
            
            weighted_votes[:, :, class_id] += vote_sum

        weighted_votes /= torch.clamp(weighted_votes.sum(dim=-1, keepdim=True), min=1e-6)

        predictions = torch.argmax(weighted_votes, dim=-1).reshape(B, HW)

        ## üîπ **FALLBACK: Reassign High-Distance Pixels to Class 3**
        # If a pixel has a high distance to all prototypes, it is likely an invalid/cloud pixel.
        min_distances, _ = torch.min(distances, dim=-1)  # Shape: [B, HW, num_class]
        min_distances = torch.min(min_distances, dim=-1)[0]  # Get min across classes, shape: [B, HW]

        # Define a distance threshold (tune this if needed)
        distance_threshold = 0.4  # Experiment with values between 0.5 and 1.0 

        # Assign pixels with high distance to all prototypes as class 3 (cloud/invalid)
        predictions[min_distances > distance_threshold] = 3
        
        return predictions

    def interpretable_rules(self, latent_features: torch.Tensor, raw_features: torch.Tensor) ->  torch.Tensor:
        """ if then decisions based on prototype based on prototype distances and stored rules"""

        predictions = self.classify(latent_features, raw_features)
        B, HW, latent_dim = latent_features.shape
        #predictions = predictions.argmax(dim=-1)
        predictions = predictions.view(B, HW)

        coastal_band = raw_features[:, 0, :, :].reshape(B, -1)  # B01
        blue_band = raw_features[:, 1, :, :].reshape(B, -1)     # B02
        green_band = raw_features[:, 2, :, :].reshape(B, -1)    # B03
        red_band = raw_features[:, 3, :, :].reshape(B, -1)      # B04
        nir_band = raw_features[:, 7, :, :].reshape(B, -1)      # B08
        swir_band1 = raw_features[:, 10, :, :].reshape(B, -1)   # B11
        swir_band2 = raw_features[:, 11, :, :].reshape(B, -1)   # B12

        ndwi = ((green_band - nir_band) / (green_band + nir_band + 1e-6)).reshape(B, -1) # water index
        ndvi = ((nir_band - green_band) / (nir_band + green_band + 1e-6)).reshape(B, -1) # vegitation index
        
        before_counts = torch.unique(predictions, return_counts=True)
        #print(f"Before Rules: {dict(zip(before_counts[0].cpu().numpy(), before_counts[1].cpu().numpy()))}")

        if not torch.any(predictions == 3):
            print("skip rule assignment")
            return predictions  # Skip rule application if everything is already assigned

        ### **1Ô∏è‚É£ Apply Cloud Rule First (Ensuring It Doesn‚Äôt Overwrite Land or Water)**
        #print("Before Cloud Rule:", torch.unique(predictions, return_counts=True))
        cloud_mask = self.cloud_rule(swir_band1, swir_band2, blue_band, red_band, predictions)
        predictions[cloud_mask] = 2
        #print("after Cloud Rule:", torch.unique(predictions, return_counts=True))
        #print(f"after cloud Rules: {dict(zip(before_counts[0].cpu().numpy(), before_counts[1].cpu().numpy()))}")


        ### **2Ô∏è‚É£ Apply Land Rule (Ensure Land is Correctly Identified)**
        #print("Before Land Rule:", torch.unique(predictions, return_counts=True))
        land_mask = self.land_rule(ndvi, ndwi, red_band, swir_band1, swir_band2, predictions)
        predictions[land_mask] = 0  # Class 0 = Land
        #print("after land Rule:", torch.unique(predictions, return_counts=True))
        #print(f"after land Rules: {dict(zip(before_counts[0].cpu().numpy(), before_counts[1].cpu().numpy()))}")


        ### **3Ô∏è‚É£ Apply Water Rule (Ensure it Doesn't Overwrite Land)**
        #print("Before Water Rule:", torch.unique(predictions, return_counts=True))
        water_mask = self.water_rule(ndwi, swir_band1, swir_band2, blue_band, predictions)
        predictions[water_mask] = 1
        #print("after water Rule:", torch.unique(predictions, return_counts=True))
        #print(f"after water Rules: {dict(zip(before_counts[0].cpu().numpy(), before_counts[1].cpu().numpy()))}")


        ### **4Ô∏è‚É£ Correct Over-Assigned Water Pixels Using a Secondary Land Rule**
        """land_correction_mask = self.land_correction_rule(ndvi, ndwi, red_band, predictions)
        predictions[land_correction_mask] = 0  """

        ### üß† Apply Rule-Based Classification (Efficient)
        predictions = self.apply_stored_rules(latent_features, predictions)
        after_counts = torch.unique(predictions, return_counts=True)
        #print(f"After Rules: {dict(zip(after_counts[0].cpu().numpy(), after_counts[1].cpu().numpy()))}")

        #print(f"Prototype vectors shape: {self.prototype_vectors.shape}")
        #print(f"Prototype labels shape: {self.prototype_labels.shape}")
        #print(f"Unique prototype labels: {torch.unique(self.prototype_labels, return_counts=True)}")

        # 5Ô∏è‚É£ Final Refinement Using KNN Decision-Making
        if hasattr(self, "prototype_vectors") and hasattr(self, "prototype_labels"):
            predictions = self.knn_decision_making(latent_features, predictions, K=10)
            print(f"After KNN Decision-Making: {torch.unique(predictions, return_counts=True)}")
        else:
            print("‚ö† Warning: Prototypes not found. Skipping KNN refinement.")

        return predictions
    
    
    def cloud_rule(self, swir_band1, swir_band2, blue_band, red_band, predictions):
        """ Detect cloud pixels based on SWIR bands and brightness in visible spectrum """
        #cloud_mask = ((swir_band1 > 0.7) | (swir_band2 > 0.8))  # Slightly lower SWIR threshold
        cloud_mask = ((swir_band1 > 0.6) | (swir_band2 > 0.7)) & (blue_band > 0.3) & (red_band > 0.3)
        # Ensure clouds are bright in visible bands
        #bright_mask = (blue_band > 0.2) & (red_band > 0.2)  

        return cloud_mask #& bright_mask  # Cloud pixels must be bright in visible and SWIR bands
    

    ### **üåä Water Detection Rule**
    def water_rule(self, ndwi, swir_band1, swir_band2, blue_band, predictions):
        """ Detect water pixels based on NDWI and SWIR reflectance """
        #water_mask = (ndwi > 0.2) & ((swir_band1 < 0.2) & (swir_band2 < 0.2)) # Allow more shallow water detection !changed the and to an or! -> the or broke it icl
        water_mask = (ndwi > 0.15) & (swir_band1 < 0.15) & (swir_band2 < 0.15) | (blue_band > 0.35)

        #low_swir =  # Water is dark in SWIR
        #bright_blue = blue_band > 0.4  # Water reflects in blue spectrum

        return water_mask # bright_blue


    ### **üåø Land Detection Rule**
    def land_rule(self, ndvi, ndwi, red_band, swir_band1, swir_band2, predictions):
        """ Detect land pixels while avoiding confusion with water & wetlands """
        #land_mask = (ndvi > 0.18) & (red_band > 0.02) #& (ndwi < 0.05) # Capture vegetated land
        land_mask = (ndvi > 0.2) & (red_band > 0.02) & (ndwi < 0.05) & (swir_band1 < 0.3) & (swir_band2 < 0.3)

        #not_water = (ndwi < 0.05)  # Ensure it is not wet soil or water
        #not_too_moist = (swir_band1 < 0.3) & (swir_band2 < 0.3)  # Ensure it‚Äôs not saturated

        return land_mask #& not_water & not_too_moist

    def land_correction_rule(self, ndvi, ndwi, red_band, predictions):
        """Fix pixels incorrectly labeled as water if they match land characteristics."""
        land_mask = ((ndvi > 0.3) & (ndwi < 0.05) & (red_band > 0.08)) & (predictions == 1)
        return land_mask  

    """
        old metrics
        ndwi_threshold = 0.01 if class_id == 2 else 0.05  
        water -> limits 
        if ndwi > ndwi_threshold and class_id != 2:  
            predictions[b, i] = 2  # Force Water classification

        # üå• **SWIR-based cloud detection**
        if swir_band1 > 0.2 or swir_band2 > 0.2:
            print(f"‚òÅÔ∏è Overriding {b}, {i} to Cloud (SWIR: {swir_band1[b, i]:.2f})")
            predictions[b, i] = 3  # Force Cloud classification

        if ndvi[b, i] > 0.3 and class_id != 1:
            print(f"üåø Overriding class at {b}, {i} to Land (NDVI: {ndvi[b, i]:.2f})")
            predictions[b, i] = 1  # Assign Land class


        working metrics: 
        def water_rule(self, ndwi, swir_band1, swir_band2, blue_band, predictions):
        Detect water pixels based on NDWI and SWIR reflectance 
        water_mask = (ndwi > 0.2)  # Allow more shallow water detection
        low_swir = (swir_band1 < 0.2) & (swir_band2 < 0.2)  # Water is dark in SWIR
        bright_blue = blue_band > 0.1  # Water reflects in blue spectrum

        return water_mask #& low_swir & bright_blue


        ### **üåø Land Detection Rule**
        def land_rule(self, ndvi, ndwi, red_band, swir_band1, swir_band2, predictions):
            Detect land pixels while avoiding confusion with water & wetlands
            land_mask = (ndvi > 0.18) & (red_band > 0.02)  # Capture vegetated land
            not_water = (ndwi < 0.05)  # Ensure it is not wet soil or water
            not_too_moist = (swir_band1 < 0.3) & (swir_band2 < 0.3)  # Ensure it‚Äôs not saturated

            return land_mask #& not_water & not_too_moist

        def land_correction_rule(self, ndvi, ndwi, red_band, predictions):
        Fix pixels incorrectly labeled as water if they match land characteristics.
            land_mask = ((ndvi > 0.3) & (ndwi < 0.05) & (red_band > 0.08)) & (predictions == 1)
            return land_mask 
            """

    ### **üß† Efficiently Apply Stored Rules**
    def apply_stored_rules(self, latent_features, predictions):
        """Efficiently applies stored IF-THEN rules using batch processing."""
        if not self.rules:
            return predictions  # Skip if no rules exist

        B, HW, latent_dim = latent_features.shape
        latent_features_flat = latent_features.view(B * HW, latent_dim)
        predicted_classes = predictions.view(-1)

        # Tensor-based rule mask
        rule_mask = torch.zeros_like(predicted_classes, dtype=torch.bool)
        adjusted_classes = predicted_classes.clone()

        for class_id, rules in self.rules.items():
            if not rules:
                continue  # Skip if no rules for this class

            matches = torch.zeros_like(predicted_classes, dtype=torch.bool)

            for rule in rules:
                rule_conditions = torch.ones_like(predicted_classes, dtype=torch.bool)

                for feature_idx, condition in enumerate(rule):
                    min_val, max_val = condition["min"], condition["max"]
                    feature_vals = latent_features_flat[:, feature_idx]

                    rule_conditions &= (feature_vals >= min_val) & (feature_vals <= max_val)

                matches |= rule_conditions

            rule_mask |= matches
            adjusted_classes[matches] = class_id

        predictions.view(-1)[rule_mask] = adjusted_classes[rule_mask]
        return predictions

    def knn_decision_making(self, latent_features, predictions, K=2): # 10 over assigns to water, 5 over assigns to cloud, 1 = just land, 2 = just water, 3 is strange,3 isd good ma but bad wateer
        """Implements KNN voting for final classification."""
        B, HW, latent_dim = latent_features.shape
        latent_features = latent_features.contiguous()
        latent_features_flat = latent_features.reshape(B * HW, latent_dim).cpu().numpy()

        # Ensure prototypes and labels exist
        if not hasattr(self, "prototype_vectors") or not hasattr(self, "prototype_labels"):
            print("‚ö† KNN Warning: Prototypes missing. Using rule-based predictions.")
            return predictions.view(B, HW)  # Return the current rule-based predictions
        #print("üìä Unique prototype labels:", torch.unique(self.prototype_labels, return_counts=True))
        knn = KNeighborsClassifier(n_neighbors=K, metric='euclidean')
        knn.fit(self.prototype_vectors.view(-1, latent_dim).cpu().numpy(), self.prototype_labels.cpu().numpy())
        
        distances, predicted_labels = knn.kneighbors(latent_features_flat, return_distance=True)

        # Ensure predictions only belong to valid classes (0,1,2,3)
        valid_classes = torch.tensor([0, 1, 2, 3], device=latent_features.device)
        predicted_labels = torch.tensor(predicted_labels[:, 0], device=latent_features.device)

        # Map invalid predictions to nearest valid class
        predicted_labels[~torch.isin(predicted_labels, valid_classes)] = 3  # Default to 'cloud' if invalid

        # Only override predictions where KNN is confident
        distance_threshold = 0.4 # was on 3  
        confident_mask = distances[:, 0] < distance_threshold  

        new_predictions = predicted_labels.clone()

        # Keep original class where KNN is unsure
        final_predictions = predictions.clone()
        final_predictions.view(-1)[confident_mask] = new_predictions[confident_mask]

        return final_predictions.view(B, HW)


    def compute_losses(self, latent_features: torch.Tensor, labels: torch.Tensor):
        """
        Compute CrossEntropy loss and prototype loss with improved stability
        """
        B, HW, latent_dim = latent_features.shape
        H, W = int(HW**0.5), int(HW**0.5)

        # Clean inputs
        latent_features = torch.nan_to_num(latent_features, nan=0.0).to(dtype=torch.float32)
        self.prototype_vectors.data.copy_(torch.nan_to_num(self.prototype_vectors, nan=0.0).to(dtype=torch.float32))
        prototypes = self.prototype_vectors.view(-1, latent_dim)

        # Compute distances once
        distances = torch.cdist(latent_features.reshape(-1, latent_dim), prototypes, p=2)
        distances = distances.view(B, HW, self.num_class, self.num_prototypes)
        distances = torch.clamp(distances, min=1e-6, max=100).to(dtype=torch.float32)

        # Prepare labels
        labels = torch.clamp(labels, min=0, max=self.num_class - 1)
        valid_mask = (labels != -1)
        labels_valid = labels * valid_mask

        # Compute logits for all classes at once
        logits = -distances.mean(dim=-1)  # [B, HW, num_class]
        logits = logits.permute(0, 2, 1).view(B, self.num_class, H, W)
        logits = torch.clamp(logits, min=-50, max=50)

        # CrossEntropy loss
        ce_loss = F.cross_entropy(
            logits,
            labels_valid.view(B, H, W),
            weight=self.weight_per_class.to(logits.device),
            ignore_index=0,  # Ignore invalid pixels
            reduction='none'
        )
        ce_loss = (ce_loss * valid_mask.float()).sum() / (valid_mask.sum() + 1e-6)

        # Prototype loss
        prototype_loss = 0.0
        for class_id in range(self.num_class):
            class_mask = (labels_valid == class_id).float().view(B, -1, 1)
            if class_mask.sum() > 0:
                class_distances = distances[:, :, class_id]
                prototype_loss += ((class_distances * class_mask).sum() / (class_mask.sum() + 1e-6))

        # Weight the losses
        total_loss = ce_loss + 0.1 * prototype_loss

        # Debug info
        if torch.isnan(total_loss):
            print(f"‚ö†Ô∏è NaN detected! CE Loss: {ce_loss}, Prototype Loss: {prototype_loss}")
            print(f"Valid pixels: {valid_mask.sum()}/{valid_mask.numel()}")
            print(f"Logits range: [{logits.min():.3f}, {logits.max():.3f}]")

        return total_loss, ce_loss, prototype_loss

    """def apply_rule(self, class_id: int, feature_vals: List[float]) -> bool:
         checks if featuer set matches stored rules for class 
        rules = self.rules.get(class_id, [])
        
        match_scores = []

        for rule in rules:
            num_conditions_met = sum(
                rule[idx]["min"] <= feature_vals[idx] <= rule[idx]["max"]
                for idx in range(len(rule))
        )
        if len(rule) > 0:  
            confidence = num_conditions_met / len(rule)  # % of conditions met
            match_scores.append(confidence)

        return max(match_scores) >= 0.8 if match_scores else False"""
    
    def add_rule(self,class_id: int, feature_ranges: List[Tuple[float, float]]):
        """ Automatically generates rules based on prototype variance """
        prototype_vectors = self.prototype_vectors[class_id]
        band_names = [f"feature_{i}" for i in range(self.latent_dim)]
        mean_proto = prototype_vectors.mean(dim=0)
        std_proto = prototype_vectors.std(dim=0)

        feature_ranges = [(mean_proto[i] - std_proto[i], mean_proto[i] + std_proto[i]) 
                          for i in range(len(mean_proto))]

        if class_id not in self.rules:
            self.rules[class_id] = []

        new_rule = [{"min": feature_range[0], "max": feature_range[1]} for feature_range in feature_ranges]
        self.rules[class_id].append(new_rule)

        conditions = [f"({band_names[i]} ~ {feature['min']:.3f}-{feature['max']:.3f})"
                  for i, feature in enumerate(new_rule)]
    
        rule_text = f"IF {' AND '.join(conditions)} THEN Class {class_id}"
        
        # ‚úÖ Store the rule for later CSV saving
        self.rule_storage.append((class_id, rule_text))

    def save_rules(self, filename="D:\Documents\coding\300\ml4floods\300_Project\if-then-rules\if_then_rules.csv"):
        """ Save all IF-THEN rules in readable format to a CSV """
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        with open(filename, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["Class ID", "IF-THEN Rule"])  # Header

            for class_id, rule_text in self.rule_storage:
                writer.writerow([class_id, rule_text])
        
        print(f"‚úÖ IF-THEN rules saved to {filename}")

    def forward(self, x:torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (B, num_channels, H, W) input tensor

        Returns:
            (B, 3, H, W) prediction of the network
        """
        #changes
        latent_features = self.extract_latent_features(x)
        raw_features = x
        rule_based_predictions = self.interpretable_rules(latent_features, raw_features)
        return rule_based_predictions
    

    def image_to_logger(self, x:torch.Tensor) -> Optional[np.ndarray]:
        return batch_to_unnorm_rgb(x,
                                   self.hparams["model_params"]["hyperparameters"]['channel_configuration'],
                                   unnormalize=self.normalized_data)


    def log_images(self, x, y, logits, prefix=""):
        import wandb
        mask_data = y.cpu().numpy()
        pred_data = torch.argmax(logits, dim=1).long().cpu().numpy()

        img_data = self.image_to_logger(x)

        if img_data is not None:
            self.logger.experiment.log(
                {f"{prefix}overlay": [self.wb_mask(img, pred, mask) for (img, pred, mask) in zip(img_data, pred_data, mask_data)]})
            self.logger.experiment.log({f"{prefix}image": [wandb.Image(img) for img in img_data]})

        self.logger.experiment.log({f"{prefix}y": [wandb.Image(mask_to_rgb(img)) for img in mask_data]})
        self.logger.experiment.log({f"{prefix}pred": [wandb.Image(mask_to_rgb(img + 1)) for img in pred_data]})

    def validation_step(self, batch: Dict, batch_idx):
        """
        Args:
            batch: includes
                x (torch.Tensor): (B, C, W, H), input image
                y (torch.Tensor): (B, W, H) encoded as {0: invalid, 1: land, 2: water, 3: cloud}
        """
        x, y = batch['image'], batch['mask'].squeeze(1)
        logits = self.network(x)
        
        bce_loss = losses.cross_entropy_loss_mask_invalid(logits, y, weight=self.weight_per_class.to(self.device))
        dice_loss = losses.dice_loss_mask_invalid(logits, y)
        self.log('val_bce_loss', bce_loss)
        self.log('val_dice_loss', dice_loss)

        pred_categorical = torch.argmax(logits, dim=1).long()

        # cm_batch is (B, num_class, num_class)
        cm_batch = metrics.compute_confusions(y, pred_categorical, num_class=self.num_class,
                                              remove_class_zero=True)

        # Log accuracy per class
        recall = metrics.calculate_recall(cm_batch, self.label_names)
        for k in recall.keys():
            self.log(f"val_recall {k}", recall[k])

        # Log IoU per class
        iou_dict = metrics.calculate_iou(cm_batch, self.label_names)
        for k in iou_dict.keys():
            iou_value = iou_dict[k]
            iou_value = max(iou_value, 0) # ensure it isn't negative
            self.log(f"val_iou {k}", iou_dict[k])
            
        if (batch_idx == 0) and (self.logger is not None) and isinstance(self.logger, WandbLogger):
            self.log_images(x, y, logits,prefix="val_")

    def visualize_prototypes(self, epoch: int):
        """
        Visualize and log prototypes for interpretability.
        """
        # Ensure prototypes exist
        if not hasattr(self, "prototype_vectors") or self.prototype_vectors is None:
            print("No prototypes found to visualize.")
            return

        prototypes = self.prototype_vectors  # Shape: [num_classes * num_prototypes, latent_dim]
        num_classes = self.num_class
        num_prototypes_per_class = prototypes.shape[0] // num_classes

        fig, axs = plt.subplots(num_classes, num_prototypes_per_class, figsize=(15, 5))
        for class_id in range(num_classes):
            for proto_id in range(num_prototypes_per_class):
                prototype = prototypes[class_id * num_prototypes_per_class + proto_id].detach().cpu().numpy()

                ax = axs[class_id, proto_id] if num_prototypes_per_class > 1 else axs[class_id]
                ax.imshow(prototype, cmap="viridis")
                ax.set_title(f"Class {class_id}, Proto {proto_id}")
                ax.axis("off")

        plt.tight_layout()

        # Save the figure as an image
        writer = SummaryWriter(log_dir=self.logger.log_dir)
        writer.add_figure(f"Prototypes/Epoch_{epoch}", fig)
        writer.close()

        plt.close(fig)

        # Visualize KMeans cluster centers
        if hasattr(self, "kmeans") and self.kmeans is not None:
            cluster_centers = self.kmeans.cluster_centers_
            fig, ax = plt.subplots(figsize=(10, 6))

            for i, center in enumerate(cluster_centers):
                ax.plot(center, label=f"Cluster {i}")

            ax.set_title("KMeans Cluster Centers")
            ax.legend()
            plt.tight_layout()

            # Save KMeans visualizations
            writer.add_figure(f"KMeans/Epoch_{epoch}", fig)
            writer.close()
            plt.close(fig)

        else:
            print("No KMeans model found to visualize.")


    def visualize_clusters(self):
        num_features = self.prototype_vectors.shape[-1]

        fig, axes = plt.subplots(1, self.num_class, figsize=(15, 5))

        for class_id in range(self.num_class):
            prototypes = self.prototype_vectors[class_id].detach().cpu().numpy()
            
            # Compute PCA for visualization (reduce dimensionality)
            pca = PCA(n_components=2)
            reduced_prototypes = pca.fit_transform(prototypes)

            # Plot prototypes
            ax = axes[class_id]
            ax.scatter(reduced_prototypes[:, 0], reduced_prototypes[:, 1], alpha=0.7)
            ax.set_title(f"Class {class_id} Prototypes")
            ax.set_xlabel("PCA Component 1")
            ax.set_ylabel("PCA Component 2")

        plt.suptitle("Prototype Clusters for Each Class")
        plt.show(block=False)
        plt.pause(0.001)

    def on_train_epoch(self):
        """
        Hook to visualize prototypes after each epoch.
        """
        super().on_train_epoch_end()
        super().on_train_epoch_start()
        self.visualize_prototypes(epoch=self.current_epoch)

    def on_train_epoch_start(self): 
        super().on_train_epoch_start()
        #self.visualize_clusters()
        # Reset metric storage for the new epoch
        self.train_epoch_metrics = {
            "train_total_loss": [],
            "train_bce_loss": [],
            "train_prototype_loss": [],
            "train_recall_land": [],
            "train_recall_water": [],
            "train_recall_cloud": [],
            "train_iou_land": [],
            "train_iou_water": [],
            "train_iou_cloud": [],
        }

    def on_train_epoch_end(self):
        super().on_train_epoch_end()
        #self.visualize_clusters()
        top_k = 20
        avg_metrics = {}
        for key, value in self.train_epoch_metrics.items():
            if len(value) > 0:
                # Sort values in descending order and take top-K
                top_k_values = sorted(value, reverse=True)[:top_k]
                avg_metrics[key] = np.mean(top_k_values)
            else:
                avg_metrics[key] = 0.0  # Default value if empty

        # Log the filtered best training metrics
        for metric_name, metric_value in avg_metrics.items():
            self.log(metric_name, metric_value, prog_bar=False)

        print("\nEpoch {} metrics:".format(self.current_epoch))
        for metric_name, metric_value in avg_metrics.items():
            print(f"{metric_name}: {metric_value:.3f}")

        self.save_rules()
            

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.network.parameters(), self.lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,
                                                               mode=METRIC_MODE[self.hparams["model_params"]["hyperparameters"]["metric_monitor"]],
                                                               factor=self.lr_decay, verbose=True,
                                                               patience=self.lr_patience)

        return {"optimizer": optimizer, "lr_scheduler": scheduler,
                "monitor": self.hparams["model_params"]["hyperparameters"]["metric_monitor"]}

    def wb_mask(self, bg_img, pred_mask, true_mask):
        import wandb
        return wandb.Image(bg_img, masks={
            "prediction" : {"mask_data" : pred_mask, "class_labels" : self.labels()},
            "ground truth" : {"mask_data" : true_mask, "class_labels" : self.labels()}})
        
    def labels(self):
        return {
            0: "invalid",
            1: "land",
            2: "water",
            3: "cloud"
        }


METRIC_MODE = {
    "val_dice_loss": "min",
    "val_iou water": "max",
    "val_bce_land_water": "min",
    "val_bce_loss": "min",
    "val_Acc_land_water": "max",
    "train_iou_water": "max"
}

"""
METRIC_MODE = {
    "val_dice_loss": "min",
    "val_iou water": "max",
    "val_bce_land_water": "min",
    "val_bce_loss": "min",
    "val_Acc_land_water": "max"
}


def classify(self, latent_features, raw_features):
        use protoype distances and interpretable rules to class pixels 
        B, HW, latent_dim = latent_features.shape
        latent_features = latent_features.contiguous().reshape(B * HW, latent_dim)

        self.prototype_vectors = torch.nn.Parameter(torch.nan_to_num(self.prototype_vectors, nan=0.0).to(dtype=torch.float32))

        distances = torch.cdist(
            torch.nan_to_num(latent_features, nan=0.0).to(dtype=torch.float32), 
            torch.nan_to_num(self.prototype_vectors.view(-1, latent_dim), nan=0.0)
        )
        distances = distances.view(B, HW, self.num_class, self.num_prototypes)

        # Find the k-nearest prototypes for each pixel
        _, nearest_prototypes = distances.topk(k=10, dim=-1, largest=False)
        class_votes = nearest_prototypes // self.num_prototypes  # Convert prototype index to class
        predictions = torch.mode(class_votes, dim=-1)[0]  # Majority vote

        # Compute NDWI (Normalized Difference Water Index)
        green_band = raw_features[:, 2, :, :].reshape(B, -1)  # Assume Band 2 is Green
        nir_band = raw_features[:, 7, :, :].reshape(B, -1)  # Assume Band 7 is NIR
        ndwi = (green_band - nir_band) / (green_band + nir_band + 1e-6)

        # Apply NDWI-based rule for water detection
        water_threshold = 0.2  # Adjust based on dataset
        water_mask = ndwi > water_threshold
        predictions[water_mask] = 2  # Force Water classification

        print(f"üîπ Class Distribution: {torch.bincount(predictions.view(-1))}")
        return predictions
"""

class ML4FloodsModel(pl.LightningModule):
    """
    Model to do multioutput binary classification.
    It expects ground truths y (B, 2, H, W) tensors to be encoded as:
    - Channel 0: {0: invalid, 1: clear, 2: cloud}
    - Channel 1: {0: invalid, 1: land, 2: water}
    """
    def __init__(self, model_params: AttrDict, normalized_data:bool = True):
        super().__init__()
        self.save_hyperparameters()
        h_params_dict = model_params.get('hyperparameters', {})
        self.num_class = h_params_dict.get('num_classes', 2)
        assert self.num_class == 2, "Expected 2 output classes"

        self.pos_weight = h_params_dict.get('pos_weight',
                                            [1 for i in range(self.num_class)])
        self.weight_problem = [1 / self.num_class for _ in range(self.num_class)]

        self.network = configure_architecture(h_params_dict)
        self.normalized_data = normalized_data

        # learning rate params
        self.lr = h_params_dict.get('lr', 1e-4)
        self.lr_decay = h_params_dict.get('lr_decay', 0.5)
        self.lr_patience = h_params_dict.get('lr_patience', 2)

        # label names setup
        self.label_names = np.array(h_params_dict['label_names'])
        assert self.label_names.shape == (2, 3), "Unexpected label names, expected: {}".format([["invalid","clear", "cloud"],
                                                                                                ["invalid", "land", "water"]])

        self.colormaps = {0 : COLORS_WORLDFLOODS_INVCLEARCLOUD, 1: COLORS_WORLDFLOODS_INVLANDWATER}


    def training_step(self, batch: Dict, batch_idx) -> float:
        """
        Args:
            batch: includes
                x (torch.Tensor): (B, 2, W, H), input image
                y (torch.Tensor): (B, 2, W, H) encoded as {0: invalid, 1: neg_xxx, 2: pos_xxx}
        """
        x, y = batch['image'], batch['mask'].squeeze(1)
        logits = self.network(x)
        loss = losses.calc_loss_multioutput_logistic_mask_invalid(logits, y,
                                                                  pos_weight_problem=self.pos_weight,
                                                                  weight_problem=self.weight_problem)

        if (batch_idx % 100) == 0:
            self.log("loss", loss)

        if (batch_idx == 0) and (self.logger is not None) and isinstance(self.logger, WandbLogger):
            with torch.no_grad():
                self.log_images(x, y, logits, prefix="train_")

        return loss

    def forward(self, x):
        """

        Args:
            x: (B, num_channels, H, W) input tensor

        Returns:
            (B, 2, H, W) prediction of the network:
            - channel 0: probability of pixel being cloud
            - channel 1: probability of pixel being cwater

        """
        return self.network(x)

    def image_to_logger(self, x:torch.Tensor) -> Optional[np.ndarray]:
        return batch_to_unnorm_rgb(x,
                                   self.hparams["model_params"]["hyperparameters"]['channel_configuration'],
                                   unnormalize=self.normalized_data)

    def log_images(self, x, y, logits, prefix=""):
        import wandb
        """ Log batch images and preds using wandb """
        mask_data = y.cpu().numpy()
        pred_data = torch.round(torch.sigmoid(logits)).long().cpu().numpy()
        img_data = self.image_to_logger(x)

        self.logger.experiment.log({f"{prefix}image": [wandb.Image(img) for img in img_data]})

        for i in range(self.num_class):
            problem_name = "_".join(self.label_names[i, 1:])
            self.logger.experiment.log({f"{prefix}{problem_name}_pred_cont": [wandb.Image(img[i], mode="L") for img in pred_data]})
            self.logger.experiment.log({f"{prefix}{problem_name}_pred": [wandb.Image(mask_to_rgb(img[i].round().astype(np.int64) + 1,
                                                                                                 values=[0, 1, 2], colors_cmap=self.colormaps[i])) for img in pred_data]})
            self.logger.experiment.log({f"{prefix}y_{problem_name}": [wandb.Image(mask_to_rgb(img[i], values=[0, 1, 2], colors_cmap=self.colormaps[i])) for img in mask_data]})

    def validation_step(self, batch: Dict, batch_idx):
        """
        Args:
            batch: includes
                x (torch.Tensor): (B, C, W, H), input image
                y (torch.Tensor): (B, W, H) encoded as {0: invalid, 1: land, 2: water, 3: cloud}
        """
        x, y = batch['image'], batch['mask']
        logits = self.network(x)

        bce_loss = losses.calc_loss_multioutput_logistic_mask_invalid(logits, y)
        self.log('val_bce_loss', bce_loss)

        pred_categorical = torch.round(torch.sigmoid(logits)).long()

        # cm_batch is (B, num_class, num_class)

        for i in range(len(self.label_names)):
            cm_batch = metrics.compute_confusions(y[:, i], pred_categorical[:, i],
                                                  num_class=2, remove_class_zero=True)

            problem_name = "_".join(self.label_names[i, 1:])

            cm_agg = torch.sum(cm_batch, dim=0)
            self.log(f"val_Acc_{problem_name}", metrics.binary_accuracy(cm_agg))
            self.log(f"val_Precision_{problem_name}", metrics.binary_precision(cm_agg))
            self.log(f"val_Recall_{problem_name}", metrics.binary_recall(cm_agg))

            bce = losses.binary_cross_entropy_loss_mask_invalid(logits[:, i], y[:, i], pos_weight=None)
            self.log(f"val_bce_{problem_name}", bce)

            # Log IoU per class
            iou_dict = metrics.calculate_iou(cm_batch, self.label_names[i, 1:])
            for k in iou_dict.keys():
                self.log(f"val_iou_{problem_name} {k}", iou_dict[k])

        if (batch_idx == 0) and (self.logger is not None) and isinstance(self.logger, WandbLogger):
            self.log_images(x, y, logits, prefix="val_")

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.network.parameters(), self.lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,
                                                               mode=METRIC_MODE[self.hparams["model_params"]["hyperparameters"]["metric_monitor"]],
                                                               factor=self.lr_decay, verbose=True,
                                                               patience=self.lr_patience)

        return {"optimizer": optimizer, "lr_scheduler": scheduler,
                "monitor": self.hparams["model_params"]["hyperparameters"]["metric_monitor"]}


def mask_to_rgb(mask:np.ndarray, values:List[int]=[0, 1, 2, 3],
                colors_cmap:np.ndarray=COLORS_WORLDFLOODS) -> np.ndarray:
    """
     Given a 2D mask it assign each value of the mask the corresponding color
    Args:
        mask: (H, W) tensor with values in values
        values: list of values to replace by the colors in colors_cmap
        colors_cmap:

    Returns:

    """
    assert len(values) == len(
        colors_cmap), f"Values and colors should have same length {len(values)} {len(colors_cmap)}"
    assert len(mask.shape) == 2, f"Unexpected shape {mask.shape}"
    mask_return = np.zeros(mask.shape[:2] + (3,), dtype=np.uint8)
    colores = np.array(np.round(colors_cmap * 255), dtype=np.uint8)
    for i, c in enumerate(colores):
        mask_return[mask == values[i], :] = c
    return mask_return


PATH_TO_MODEL_HRNET_SMALL = "gs://ml4cc_data_lake/0_DEV/2_Mart/2_MLModelMart/hrnet_small_imagenet/HRNet_W18_C_ssld_pretrained.pth"


def configure_architecture(h_params:AttrDict) -> torch.nn.Module:
    architecture = h_params.get('model_type', 'linear')
    num_channels = h_params.get('num_channels', 3)
    num_classes = h_params.get('num_classes', 2)

    if architecture == 'unet':
        model = UNet(num_channels, num_classes)
    
    elif architecture == 'segnet':
        model = SegNet(num_channels, num_classes)

    elif architecture == 'simplecnn':
        model = SimpleCNN(num_channels, num_classes)

    elif architecture == 'linear':
        model = SimpleLinear(num_channels, num_classes)

    elif architecture == 'unet_dropout':
        model = UNet_dropout(num_channels, num_classes)

    elif architecture == "hrnet_small":
        model = HighResolutionNet(input_channels=num_channels, output_channels=num_classes)
        if num_channels == 3:
            print("3-channel model. Loading pre-trained weights from ImageNet")
            pretrained_dict = load_weights(PATH_TO_MODEL_HRNET_SMALL)
            model.init_weights(pretrained_dict)

    else:
        raise Exception(f'No model implemented for model_type: {h_params.model_type}')

    return model


def load_weights(path_weights:str, map_location="cpu"):
    fs = get_filesystem(path_weights)
    if fs.exists(path_weights):
        with fs.open(path_weights, "rb") as fh:
            weights = torch.load(fh, map_location=map_location)

        return weights

    raise ValueError(f"Pretrained weights file: {path_weights} does not exists")


def batch_to_unnorm_rgb(x:torch.Tensor, channel_configuration:str="all", max_clip_val=3000.,
                        unnormalize:bool=True) -> Optional[np.ndarray]:
    """
    Unnorm x images and get rgb channels for visualization

    Args:
        x: (B, C, H, W) image
        channel_configuration: one of CHANNELS_CONFIGURATIONS.keys()
        max_clip_val: value to saturate the image
        unnormalize:

    Returns:
        (B, H, W, 3) np.array with values between 0-1 ready to be used in imshow/PIL.from_array()
    """
    model_input_npy = x.cpu().numpy()

    # Find the RGB indexes within the S2 bands
    bands_read_names = [BANDS_S2[i] for i in CHANNELS_CONFIGURATIONS[channel_configuration]]
    bands_index_rgb = [bands_read_names.index(b) for b in ["B4", "B3", "B2"] if b in bands_read_names]
    if len(bands_index_rgb) != 3:
        # try swir/nir/red
        bands_index_rgb = [bands_read_names.index(b) for b in ["B11", "B8", "B4"] if b in bands_read_names]
        if len(bands_index_rgb) != 3:
            return

    mean, std = normalize.get_normalisation("rgb")  # B, R, G!
    mean = mean[np.newaxis] # (1, 1, 1, nchannels)
    std = std[np.newaxis] # (1, 1, 1, nchannels)

    model_input_rgb_npy = model_input_npy[:, bands_index_rgb, ...].transpose(0, 2, 3, 1)
    if unnormalize:
        model_input_rgb_npy = model_input_rgb_npy  * std + mean
        model_input_rgb_npy = np.clip(model_input_rgb_npy / max_clip_val, 0., 1.)

    return model_input_rgb_npy


def unnorm_batch(x:torch.Tensor, channel_configuration:str="all", max_clip_val:float=3000.) ->torch.Tensor:
    model_input_npy = x.cpu().numpy()

    mean, std = normalize.get_normalisation(channel_configuration, channels_first=True)
    mean = mean[np.newaxis] # (1, nchannels, 1, 1)
    std = std[np.newaxis]  # (1, nchannels, 1, 1)
    out = model_input_npy * std + mean
    if max_clip_val is not None:
        out = np.clip(out/max_clip_val, 0, 1)
    return out


def plot_batch(x:torch.Tensor, channel_configuration:str="all", bands_show=None, axs=None, max_clip_val=3000.,
               show_axis=False):
    """

    Args:
        x:
        channel_configuration:
        bands_show: RGB ["B4", "B3", "B2"] SWIR/NIR/RED ["B11", "B8", "B4"]
        axs:
        max_clip_val: value to saturate the image
        show_axis: Whether to show axis of the image or not

    Returns:

    """
    import matplotlib.pyplot as plt

    if bands_show is None:
        bands_show = ["B4", "B3", "B2"]

    if axs is None:
        fig, axs = plt.subplots(1, len(x))

    x = unnorm_batch(x, channel_configuration, max_clip_val=max_clip_val)
    bands_read_names = [BANDS_S2[i] for i in CHANNELS_CONFIGURATIONS[channel_configuration]]
    bands_index_rgb = [bands_read_names.index(b) for b in bands_show]
    x = x[:, bands_index_rgb]

    if hasattr(x, "cpu"):
        x = x.cpu()

    for xi, ax in zip(x, axs):
        xi = np.transpose(xi, (1, 2, 0))
        ax.imshow(xi)
        if not show_axis:
            ax.axis("off")
        


def plot_batch_output_v1(outputv1: torch.Tensor, axs=None, legend=True, show_axis=False):
    """

    Args:
        outputv1:  (B, W, H) Tensor encoded as {0: invalid, 1: land, 2: water, 3: cloud}
        axs:
        legend: whether to show the legend or not
        show_axis:  Whether to show axis of the image or not

    Returns:

    """
    import matplotlib.pyplot as plt
    from ml4floods.visualization import plot_utils

    if hasattr(outputv1, "cpu"):
        outputv1 = outputv1.cpu()

    if axs is None:
        axs = plt.subplots(1, len(outputv1))

    cmap_preds, norm_preds, patches_preds = plot_utils.get_cmap_norm_colors(plot_utils.COLORS_WORLDFLOODS,
                                                                            plot_utils.INTERPRETATION_WORLDFLOODS)

    for _i, (xi, ax) in enumerate(zip(outputv1, axs)):
        ax.imshow(xi, cmap=cmap_preds, norm=norm_preds,
                  interpolation='nearest')

        if not show_axis:
            ax.axis("off")

        if _i == (len(outputv1)-1) and legend:
            ax.legend(handles=patches_preds,
                      loc='upper right')


"""
legacy 

def compute_losses(self, latent_features, labels):
        
        #Compute BCE and prototype losses.
        
        # Compute distances between latent features and prototypes
        B,  HW, latent_dim = latent_features.shape
        H, W = int(HW**0.5), int(HW**0.5)
        latent_features = latent_features.reshape(-1, latent_dim)
        prototypes = self.prototype_vectors.view(-1, latent_dim)

        distances = torch.cdist(latent_features, prototypes, p=2)  # Shape: (B, H*W, num_class, num_prototypes)

        distances = distances.reshape(B, HW, self.num_class, self.num_prototypes)

        prototype_loss = 0.0
        bce_loss = 0.0

        for class_id in range(self.num_class):
            class_mask = (labels == class_id).float()
            class_distances = distances[:, :, class_id]

            # BCE Loss
            logits = -class_distances.mean(dim=-1)  # Shape: (B, HW)
            logits = logits.view(B, 1, H, W) # Reshape to 4D (B, 1, H, W)


            class_bce_loss = losses.cross_entropy_loss_mask_invalid(
                logits, class_mask.view(B, H, W).long(), weight=None
            ) # adjust for 4D shape
            #print(f"Class {class_id} BCE loss: {class_bce_loss}")
            bce_loss += class_bce_loss

            # Prototype loss (penalize high distances for correct class)
            if class_distances[class_mask.view(B, -1) > 0].numel() > 0:
                prototype_loss += class_distances[class_mask.view(B, -1) > 0].mean()
        #print(f"Total BCE loss: {bce_loss}, Total Prototype loss: {prototype_loss}")
        total_loss = bce_loss + prototype_loss
        return total_loss, bce_loss, prototype_loss
"""